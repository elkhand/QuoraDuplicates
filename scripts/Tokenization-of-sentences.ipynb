{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "#from nltk.tokenize import word_tokenize\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from enchant.checker import SpellChecker\n",
    "\n",
    "# https://www.packtpub.com/mapt/book/application-development/9781782167853/2/ch02lvl1sec23/spelling-correction-with-enchant\n",
    "import enchant\n",
    "from nltk.metrics import edit_distance\n",
    "from nltk.tokenize.stanford import StanfordTokenizer\n",
    "import string\n",
    "import time\n",
    "import os\n",
    "\n",
    "nltk_data_dir = \"/home/elkhand/datasets/nltk_data\"\n",
    "if not os.listdir(nltk_data_dir):\n",
    "    nltk.download(download_dir=nltk_data_dir,quiet=True) # Download once\n",
    "nltk.data.path.append(nltk_data_dir)\n",
    "\n",
    "rootPathOfGlove = \"/home/elkhand/datasets/glove-vectors/\"\n",
    "\n",
    "gloveVectorPath = '/home/elkhand/datasets/glove-vectors/glove.6B.300d.txt'\n",
    "\n",
    "#glvocab_300Path = \"/home/elkhand/datasets/glove-vectors/glvocab_300.txt\"\n",
    "#glwordvectors_1_300Path = \"/home/elkhand/datasets/glove-vectors/glwordvectors_1_300.txt\"\n",
    "\n",
    "quoraDataSetPath = '/home/elkhand/datasets/Quora/data/quora_duplicate_questions.tsv'\n",
    "\n",
    "derivedDataSetPathRoot = \"/home/elkhand/datasets/Quora/derived/\"\n",
    "\n",
    "EOS_CHAR = 'zzzz'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating train/dev/test set\n",
    "## conll format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'loading data...'\n",
    "quora = pd.read_csv(quoraDataSetPath, sep='\\t', encoding='utf-8')\n",
    "print quora.shape\n",
    "print quora.head(2)\n",
    "\n",
    "# replacer = SpellingReplacer()\n",
    "np.random.seed(0)\n",
    "\n",
    "def tokenize_quora_row(row, file_dict):\n",
    "    # tokenize\n",
    "    try:\n",
    "        q1_tok = word_tokenize(row['question1'])\n",
    "        q2_tok = word_tokenize(row['question2'])\n",
    "        conll_entry_a = '\\n'.join(q1_tok) + '\\n\\n'\n",
    "        conll_entry_a = conll_entry_a.encode('utf-8')\n",
    "        conll_entry_b = '\\n'.join(q2_tok) + '\\n\\n'\n",
    "        conll_entry_b = conll_entry_b.encode('utf-8')        \n",
    "        label_entry = str(row['is_duplicate'])+'\\n'\n",
    "    except Exception, e:# encoding errors, not str, etc.\n",
    "        print repr(e)  \n",
    "        return\n",
    "    \n",
    "    # split train/dev/test\n",
    "    x = np.random.random()\n",
    "    if x < 0.98: # .98\n",
    "        k = 'train'\n",
    "    elif x < 0.99: # .99\n",
    "        k = 'dev'\n",
    "    else: #0.01\n",
    "        k = 'test'\n",
    "    \n",
    "    # write\n",
    "    fa, fb, flab = file_dict[k]['fa'], file_dict[k]['fb'], file_dict[k]['flab']\n",
    "    fa.write(conll_entry_a)\n",
    "    fb.write(conll_entry_b)\n",
    "    flab.write(label_entry)\n",
    "\n",
    "    \n",
    "# tokenize conll\n",
    "with open(derivedDataSetPathRoot + 'data/dat_train_a.conll', 'w') as trainaf:\n",
    "    with open(derivedDataSetPathRoot + 'data/dat_train_b.conll', 'w') as trainbf:\n",
    "        with open(derivedDataSetPathRoot + 'data/labels_train.conll', 'w') as trainlabelf:\n",
    "            with open(derivedDataSetPathRoot + 'data/dat_dev_a.conll', 'w') as devaf:\n",
    "                with open(derivedDataSetPathRoot + 'data/dat_dev_b.conll', 'w') as devbf:\n",
    "                    with open(derivedDataSetPathRoot + 'data/labels_dev.conll', 'w') as devlabelf:        \n",
    "                        with open(derivedDataSetPathRoot + 'data/dat_test_a.conll', 'w') as testaf:\n",
    "                            with open(derivedDataSetPathRoot + 'data/dat_test_b.conll', 'w') as testbf:\n",
    "                                with open(derivedDataSetPathRoot + 'data/labels_test.conll', 'w') as testlabelf:\n",
    "                                    file_dict = {\n",
    "                                        'train': {'fa': trainaf, 'fb': trainbf, 'flab': trainlabelf},\n",
    "                                        'dev': {'fa': devaf, 'fb': devbf, 'flab': devlabelf},\n",
    "                                        'test': {'fa': testaf, 'fb': testbf, 'flab': testlabelf}\n",
    "                                    }\n",
    "                                    tok_conll = quora.apply(lambda x: tokenize_quora_row(x, file_dict), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MungeGlove\n",
    "\n",
    "Generates glvocab_1.txt and glwordvectors_1_[100/300].txt from the GloVe data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates glvocab_1.txt and glwordvectors_1_[100/300].txt from the GloVe data.\n",
    "#\n",
    "# Download the GloVe data from http://nlp.stanford.edu/data/glove.6B.zip\n",
    "\n",
    "vocab = None\n",
    "\n",
    "for embed_size in [100, 300]:\n",
    "    current_vocab = []\n",
    "\n",
    "    with open(rootPathOfGlove + \"glove.6B.%dd.txt\" % embed_size, \"r\") as f_in:\n",
    "        with open(rootPathOfGlove + \"glwordvectors_%d.txt\" % embed_size, \"w\") as f_out:\n",
    "            for line in f_in:\n",
    "                cols = line.split()\n",
    "                assert len(cols) == embed_size + 1\n",
    "\n",
    "                word = cols[0]\n",
    "                current_vocab.append(word)\n",
    "                f_out.write(line[len(word)+1:])\n",
    "\n",
    "    if vocab is None:\n",
    "        vocab = current_vocab\n",
    "    else:\n",
    "        assert vocab == current_vocab\n",
    "\n",
    "assert len(vocab) == 400000\n",
    "\n",
    "with open(rootPathOfGlove + \"glwordvectors_common.txt\", \"w\") as f:\n",
    "    for word in vocab:\n",
    "        f.write(word + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spell checking\n",
    "** NOTE: ** Not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class SpellingReplacer(object):\n",
    "    def __init__(self, dict_name='en', max_dist=2):\n",
    "        self.spell_dict = enchant.Dict(dict_name)\n",
    "        self.max_dist = max_dist\n",
    "\n",
    "    def replace(self, words):\n",
    "        corrected_words = list()\n",
    "        for word in words:\n",
    "            if self.spell_dict.check(word):\n",
    "                corrected_words.append(word)\n",
    "                continue\n",
    "            suggestions = self.spell_dict.suggest(word)\n",
    "            if suggestions and edit_distance(word, suggestions[0]) <= self.max_dist:\n",
    "                corrected_words.append(suggestions[0])\n",
    "            else:\n",
    "                corrected_words.append(word)\n",
    "    return corrected_words\n",
    "\n",
    "\n",
    "class TokenizerWorker(object):\n",
    "    \n",
    "    def __init__(self, gl_vocab_index, gl, spellreplacer):\n",
    "#         self.tokenizer = tokenizer\n",
    "        self.gl_vocab_index = gl_vocab_index\n",
    "        self.gl = gl\n",
    "        self.spellreplacer = spellreplacer\n",
    "    \n",
    "    def tokenize(self, s):\n",
    "#         tokens = self.tokenizer.tokenize(s)\n",
    "#         tokens =  re.findall(r\"[\\w']+\", s)\n",
    "        try:\n",
    "            s = re.sub(r'[^\\w\\s]','', s.encode('utf-8').lower())\n",
    "        except:\n",
    "            return [s]\n",
    "        tokens = word_tokenize(s) # lower and strip punctuation\n",
    "        tokens_postprocess = tokens\n",
    "        return tokens_postprocess\n",
    "    \n",
    "    def __call__(self, s):\n",
    "\n",
    "        tokens = self.tokenize(s)\n",
    "        s_vect = np.zeros((1, self.gl.shape[1]))\n",
    "        missing_word = False\n",
    "        \n",
    "        for word in tokens: # sum glove vectors\n",
    "            try:\n",
    "                s_vect += self.gl.iloc[self.gl_vocab_index.ix[word], :]\n",
    "            except KeyError:\n",
    "                missing_word = True\n",
    "            except TypeError:\n",
    "                continue\n",
    "        \n",
    "        if missing_word: # rerun with spellcheck\n",
    "            tokens = self.spellreplacer.replace(tokens)\n",
    "            s_vect = np.zeros((1, self.gl.shape[1]))\n",
    "            for word in tokens:\n",
    "                try:\n",
    "                    s_vect += self.gl.iloc[self.gl_vocab_index.ix[word], :]\n",
    "                except (KeyError, TypeError) as e:\n",
    "                    pass\n",
    "            s = ' '.join(tokens)\n",
    "\n",
    "        return (s, s_vect)\n",
    "    \n",
    "\n",
    "replacer = SpellingReplacer()\n",
    "\n",
    "# load glove pretrained vectors\n",
    "gl = pd.read_csv(gloveVectorPath, header=None, sep='\\s+', error_bad_lines=False, quoting=csv.QUOTE_NONE, encoding='utf8')\n",
    "gl_vocab = gl.pop(0)\n",
    "gl_vocab_index = pd.DataFrame({'vocab_idx': gl_vocab.index})\n",
    "gl_vocab_index.index = gl_vocab.values\n",
    "print gl_vocab.shape, gl.shape\n",
    "gl_vocab_index.head(5)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
